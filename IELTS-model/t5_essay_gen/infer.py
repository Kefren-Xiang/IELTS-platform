# infer.py
# ç”¨è®­ç»ƒå¥½çš„ t5_essay_gen æ¨¡å‹ç”Ÿæˆé›…æ€èŒƒæ–‡

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration

MODEL_DIR = "model_output\checkpoint-4832"

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
print("[INFO] Loading model...")
tokenizer = T5Tokenizer.from_pretrained(MODEL_DIR)
model = T5ForConditionalGeneration.from_pretrained(MODEL_DIR)
model.eval().to("cuda" if torch.cuda.is_available() else "cpu")

def generate_essay(prompt: str, max_length: int = 512):
    device = next(model.parameters()).device
    input_text = f"Generate IELTS essay: {prompt}"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=512,
            do_sample=True,
            top_p=0.9,
            temperature=0.85,
            repetition_penalty=1.2,
            no_repeat_ngram_size=3
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

if __name__ == "__main__":
    while True:
        user_input = input("\nğŸ“ è¾“å…¥é›…æ€ä½œæ–‡é¢˜ç›® (æˆ–è¾“å…¥ q é€€å‡º)ï¼š\n> ").strip()
        if user_input.lower() in ["q", "quit", "exit"]:
            break

        print("\nâ³ æ­£åœ¨ç”ŸæˆèŒƒæ–‡...\n")
        result = generate_essay(user_input)
        print("ğŸ“„ èŒƒæ–‡è¾“å‡ºï¼š\n")
        print(result)
