# infer.py â€“ è¾“å…¥ä½œæ–‡åŸæ–‡ï¼Œè¾“å‡ºè‡ªåŠ¨è¯„è¯­

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
import argparse

def load_model(model_dir):
    print(f"[INFO] Loading model from Â«{model_dir}Â» â€¦")
    tokenizer = T5Tokenizer.from_pretrained(model_dir)
    model = T5ForConditionalGeneration.from_pretrained(model_dir)
    return tokenizer, model

def generate_feedback(essay, tokenizer, model, max_len=384):
    cleaned_essay = essay.strip().replace("\n", " ")
    prompt = f"Provide IELTS feedback: {cleaned_essay}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_len).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            num_beams=5,
            early_stopping=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_dir", default="feedback_model\checkpoint-1548", help="è·¯å¾„ï¼šè®­ç»ƒä¿å­˜çš„æ¨¡å‹æ–‡ä»¶å¤¹")
    parser.add_argument("--max_len", type=int, default=384)
    args = parser.parse_args()

    tokenizer, model = load_model(args.model_dir)
    model.eval()
    if torch.cuda.is_available():
        model.cuda()

    print("\nç²˜è´´æˆ–è¾“å…¥ä¸€ç¯‡å®Œæ•´ IELTS ä½œæ–‡ï¼Œç»“æŸåå›è½¦ä¸¤æ¬¡ï¼ˆç©ºè¡Œç»ˆæ­¢ï¼‰ï¼š")
    essay_lines = []
    while True:
        line = input()
        if line.strip() == "":
            break
        essay_lines.append(line)
    essay = "\n".join(essay_lines)

    print("\nâ³ æ­£åœ¨ç”Ÿæˆåé¦ˆ â€¦\n")
    feedback = generate_feedback(essay, tokenizer, model, args.max_len)
    print("ğŸ“„ è‡ªåŠ¨è¯„è¯­è¾“å‡ºï¼š\n")
    print(feedback)

if __name__ == "__main__":
    main()
